{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pretty-argentina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_built_with_cuda())\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prescribed-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic init\n",
    "MODEL_PATH=r\"AI-Aquaculturing/ForEdgetpuModels/edgetpu_koifish_1000000/koifish_detect-100w.tflite\"\n",
    "img_path = r\"AI-Aquaculturing/101.png\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "provincial-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocess(img_path, HEIGHT, WIDTH):\n",
    "    \"\"\"\n",
    "    This function is handling that tflite model input_details, and convert image to tflite format.\n",
    "    \n",
    "    It will return cv2 image and input_data which mean image was be converted.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    if img.all():\n",
    "        imH, imW, _ = img.shape\n",
    "        image_resized = cv2.resize(img, (WIDTH, HEIGHT))\n",
    "        input_data = np.expand_dims(image_resized, axis=0)\n",
    "    else:\n",
    "        return False\n",
    "    return True, input_data, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "weighted-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_interpreter(img_path, MODEL_PATH):\n",
    "    \"\"\"\n",
    "    set_interpreter mean you need set_tensor in memory and invoke the interpreter.\n",
    "    \n",
    "    this function will return interpreter to doing pridict stuff like, output_details that you can get predicted imformation about detection boxes, scores classes ....\n",
    "    \n",
    "    return img data is allow you to save original image.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    interpreter = tf.lite.Interpreter(MODEL_PATH)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    HEIGHT = input_details[0]['shape'][1]\n",
    "    WIDTH = input_details[0]['shape'][2]\n",
    "    \n",
    "    status, input_data, img=image_preprocess(\n",
    "        img_path=img_path,\n",
    "        HEIGHT=HEIGHT,\n",
    "        WIDTH=WIDTH\n",
    "    )\n",
    "    \n",
    "    if status:\n",
    "        interpreter.allocate_tensors()\n",
    "        interpreter.set_tensor(input_details[0]['index'],input_data)\n",
    "        start_time = time.time()\n",
    "        interpreter.invoke()\n",
    "        stop_time = time.time()\n",
    "    else:\n",
    "        print(\"maybe your image get some error.\")\n",
    "        \n",
    "    print(\"set time is coast:{}\".format(stop_time-start_time))\n",
    "    return interpreter, output_details, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "about-position",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(output_details):\n",
    "    return interpreter.get_tensor(output_details[0]['index'])[0], interpreter.get_tensor(output_details[2]['index'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "static-communication",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictBox(boxes, scores, img, classes=\"1\"):\n",
    "    \"\"\"\n",
    "    This function help you to draw the detection box on image and counting population for your target.\n",
    "    \"\"\"\n",
    "    Population = int()\n",
    "    for i in range(len(scores)):\n",
    "        imH, imW, _ = img.shape\n",
    "        if ((scores[i] > 0.5) and (scores[i] <= 1.0)):\n",
    "            Population = Population + 1\n",
    "            # Get bounding box coordinates and draw box\n",
    "            # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n",
    "\n",
    "            ymin = int(max(1, (boxes[i][0] * imH)))\n",
    "            xmin = int(max(1, (boxes[i][1] * imW)))\n",
    "            ymax = int(min(imH, (boxes[i][2] * imH)))\n",
    "            xmax = int(min(imW, (boxes[i][3] * imW)))\n",
    "            cv2.rectangle(img, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n",
    "            object_name = classes\n",
    "            label = '%s: %d%%' % (object_name, int(scores[i]*100))\n",
    "            labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n",
    "            label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n",
    "\n",
    "            cv2.rectangle(img, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n",
    "            cv2.putText(img, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n",
    "    return img, Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "descending-subscription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set time is coast:0.08224320411682129\n",
      "Population:5\n"
     ]
    }
   ],
   "source": [
    "interpreter, output_details, img=set_interpreter(img_path, MODEL_PATH)\n",
    "\n",
    "boxes, scores = predict(output_details)\n",
    "\n",
    "img_withBox, Population = get_predictBox(boxes, scores, img)\n",
    "cv2.imwrite(\"output.jpg\", img_withBox)\n",
    "print(\"Population:{}\".format(Population))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfServer_py37",
   "language": "python",
   "name": "tfserver_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
